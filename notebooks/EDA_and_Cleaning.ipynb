{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA and Data Cleaning Notebook\n",
        "\n",
        "This notebook performs comprehensive Exploratory Data Analysis (EDA) and data cleaning on the computer dataset.\n",
        "\n",
        "## Objectives:\n",
        "1. Load and inspect raw data\n",
        "2. Perform full EDA (shape, dtypes, missing values, distributions)\n",
        "3. Identify data quality issues\n",
        "4. Apply cleaning transformations\n",
        "5. Export cleaned dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cleaning'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, src_path)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Import cleaning utilities\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcleaning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     extract_numeric_series,\n\u001b[1;32m     22\u001b[0m     clean_storage_fields,\n\u001b[1;32m     23\u001b[0m     clean_screen_fields,\n\u001b[1;32m     24\u001b[0m     create_cleaned_cpu_gpu_columns,\n\u001b[1;32m     25\u001b[0m     clean_multilabel_series,\n\u001b[1;32m     26\u001b[0m     identify_multilabel_columns,\n\u001b[1;32m     27\u001b[0m     clean_dataframe,\n\u001b[1;32m     28\u001b[0m     get_default_config\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Set style\u001b[39;00m\n\u001b[1;32m     32\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_style(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cleaning'"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path - get the project root (parent of notebooks directory)\n",
        "# Method 1: Try to get notebook directory from current working directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Check if we're in the notebooks directory\n",
        "if os.path.basename(current_dir) == 'notebooks':\n",
        "    project_root = os.path.dirname(current_dir)\n",
        "elif 'notebooks' in current_dir:\n",
        "    # If notebooks is in the path, go up to project root\n",
        "    project_root = current_dir.split('notebooks')[0].rstrip(os.sep)\n",
        "else:\n",
        "    # Assume we're in project root\n",
        "    project_root = current_dir\n",
        "\n",
        "# Alternative method: Try to find project root by looking for src/cleaning directory\n",
        "if not os.path.exists(os.path.join(project_root, 'src', 'cleaning')):\n",
        "    # Try going up one level from current directory\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    if os.path.exists(os.path.join(parent_dir, 'src', 'cleaning')):\n",
        "        project_root = parent_dir\n",
        "    # Try current directory itself\n",
        "    elif os.path.exists(os.path.join(current_dir, 'src', 'cleaning')):\n",
        "        project_root = current_dir\n",
        "\n",
        "src_path = os.path.join(project_root, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "# Verify the path exists\n",
        "if not os.path.exists(src_path):\n",
        "    print(f\"WARNING: src path not found at {src_path}\")\n",
        "    print(f\"Current directory: {current_dir}\")\n",
        "    print(f\"Project root: {project_root}\")\n",
        "    print(\"Please make sure you're running the notebook from the correct location.\")\n",
        "else:\n",
        "    print(f\"✓ Found src directory at: {src_path}\")\n",
        "\n",
        "# Import cleaning utilities\n",
        "try:\n",
        "    from cleaning import (\n",
        "        extract_numeric_series,\n",
        "        clean_storage_fields,\n",
        "        clean_screen_fields,\n",
        "        create_cleaned_cpu_gpu_columns,\n",
        "        clean_multilabel_series,\n",
        "        identify_multilabel_columns,\n",
        "        clean_dataframe,\n",
        "        get_default_config\n",
        "    )\n",
        "    print(\"✓ Cleaning modules imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Error importing cleaning modules: {e}\")\n",
        "    print(f\"\\nTroubleshooting:\")\n",
        "    print(f\"1. Make sure src/cleaning/ directory exists at: {src_path}\")\n",
        "    print(f\"2. Check that all .py files are in src/cleaning/\")\n",
        "    print(f\"3. Verify __init__.py exists in src/cleaning/\")\n",
        "    print(f\"\\nCurrent paths:\")\n",
        "    print(f\"  - Current directory: {current_dir}\")\n",
        "    print(f\"  - Project root: {project_root}\")\n",
        "    print(f\"  - Source path: {src_path}\")\n",
        "    raise\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(f\"\\nProject root: {project_root}\")\n",
        "print(f\"Source path: {src_path}\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "data_path = '../data/'\n",
        "df_computers = pd.read_csv(f'{data_path}db_computers_2025_raw.csv')\n",
        "df_cpu = pd.read_csv(f'{data_path}db_cpu_raw.csv')\n",
        "df_gpu = pd.read_csv(f'{data_path}db_gpu_raw.csv')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA LOADED\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nComputers dataset shape: {df_computers.shape}\")\n",
        "print(f\"CPU dataset shape: {df_cpu.shape}\")\n",
        "print(f\"GPU dataset shape: {df_gpu.shape}\")\n",
        "print(f\"\\nComputers columns: {len(df_computers.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initial Data Inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic info\n",
        "print(\"=\" * 80)\n",
        "print(\"2.1 Dataset Shape and Types\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Rows: {df_computers.shape[0]}\")\n",
        "print(f\"Columns: {df_computers.shape[1]}\")\n",
        "print(f\"\\nData types:\\n{df_computers.dtypes.value_counts()}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"First 3 rows:\")\n",
        "print(\"=\" * 80)\n",
        "df_computers.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values analysis\n",
        "print(\"=\" * 80)\n",
        "print(\"2.2 Missing Values Analysis\")\n",
        "print(\"=\" * 80)\n",
        "missing = df_computers.isnull().sum()\n",
        "missing_pct = (missing / len(df_computers)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Missing %': missing_pct\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "\n",
        "print(f\"\\nColumns with missing values: {(missing > 0).sum()}\")\n",
        "print(f\"\\nTop 20 columns with most missing values:\")\n",
        "print(missing_df[missing_df['Missing Count'] > 0].head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Duplicates\n",
        "print(\"=\" * 80)\n",
        "print(\"2.3 Duplicate Rows\")\n",
        "print(\"=\" * 80)\n",
        "duplicates = df_computers.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicates}\")\n",
        "print(f\"Percentage: {(duplicates / len(df_computers)) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. EDA - Missing Values Heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create missing values heatmap (sample of columns for visibility)\n",
        "# Select columns with most missing values for visualization\n",
        "top_missing_cols = missing_df[missing_df['Missing Count'] > 0].head(30).index.tolist()\n",
        "\n",
        "if len(top_missing_cols) > 0:\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    missing_data = df_computers[top_missing_cols].isnull()\n",
        "    sns.heatmap(missing_data, yticklabels=False, cbar=True, cmap='viridis')\n",
        "    plt.title('Missing Values Heatmap (Top 30 Columns)', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../docs/missing_values_heatmap.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No missing values to visualize\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. EDA - Numeric Fields Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify numeric fields stored as strings\n",
        "print(\"=\" * 80)\n",
        "print(\"4.1 Numeric Fields Stored as Strings\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "numeric_candidates = [\n",
        "    'Pantalla_Tamaño de la pantalla',\n",
        "    'Pantalla_Diagonal de la pantalla',\n",
        "    'RAM_Memoria RAM',\n",
        "    'Disco duro_Capacidad de memoria SSD',\n",
        "    'Disco duro_Capacidad del disco duro',\n",
        "    'Procesador_Frecuencia de reloj',\n",
        "    'Medidas y peso_Peso',\n",
        "    'Alimentación_Vatios-hora',\n",
        "]\n",
        "\n",
        "print(\"\\nSample values from numeric candidate columns:\")\n",
        "for col in numeric_candidates:\n",
        "    if col in df_computers.columns:\n",
        "        sample = df_computers[col].dropna().head(3)\n",
        "        if len(sample) > 0:\n",
        "            print(f\"\\n{col}:\")\n",
        "            for val in sample:\n",
        "                print(f\"  - {val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. EDA - Categorical Fields Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Value counts for key categorical fields\n",
        "print(\"=\" * 80)\n",
        "print(\"5.1 Value Counts for Key Categorical Fields\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "categorical_cols = [\n",
        "    'Procesador_Procesador',\n",
        "    'Gráfica_Tarjeta gráfica',\n",
        "    'Sistema operativo_Sistema operativo',\n",
        "    'Serie',\n",
        "    'Tipo de producto',\n",
        "]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df_computers.columns:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Unique values: {df_computers[col].nunique()}\")\n",
        "        print(f\"  Top 10 values:\")\n",
        "        print(df_computers[col].value_counts().head(10))\n",
        "        print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Identify Data Quality Issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify multilabel fields\n",
        "print(\"=\" * 80)\n",
        "print(\"6.1 Multilabel Fields\")\n",
        "print(\"=\" * 80)\n",
        "multilabel_cols = identify_multilabel_columns(df_computers)\n",
        "print(f\"Columns with multilabel data: {len(multilabel_cols)}\")\n",
        "print(f\"\\nFirst 10 multilabel columns:\")\n",
        "for col in multilabel_cols[:10]:\n",
        "    sample = df_computers[col].dropna().head(2)\n",
        "    if len(sample) > 0:\n",
        "        print(f\"\\n{col}:\")\n",
        "        for val in sample:\n",
        "            print(f\"  - {val}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Storage format issues\n",
        "print(\"=\" * 80)\n",
        "print(\"6.2 Storage Format Issues\")\n",
        "print(\"=\" * 80)\n",
        "storage_col = 'Disco duro_Capacidad de memoria SSD'\n",
        "if storage_col in df_computers.columns:\n",
        "    sample = df_computers[storage_col].dropna().head(10)\n",
        "    print(f\"\\nSample values from {storage_col}:\")\n",
        "    for val in sample:\n",
        "        print(f\"  - {val}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Screen size issues\n",
        "print(\"=\" * 80)\n",
        "print(\"6.3 Screen Size Issues\")\n",
        "print(\"=\" * 80)\n",
        "screen_col = 'Pantalla_Tamaño de la pantalla'\n",
        "if screen_col in df_computers.columns:\n",
        "    sample = df_computers[screen_col].dropna().head(10)\n",
        "    print(f\"\\nSample values from {screen_col}:\")\n",
        "    for val in sample:\n",
        "        print(f\"  - {val}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CPU/GPU naming issues\n",
        "print(\"=\" * 80)\n",
        "print(\"6.4 CPU/GPU Naming Issues\")\n",
        "print(\"=\" * 80)\n",
        "cpu_col = 'Procesador_Procesador'\n",
        "if cpu_col in df_computers.columns:\n",
        "    sample = df_computers[cpu_col].dropna().head(10)\n",
        "    print(f\"\\nSample CPU names:\")\n",
        "    for val in sample:\n",
        "        print(f\"  - {val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Cleaning\n",
        "\n",
        "Now we'll apply all cleaning transformations step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with a copy of the original dataframe\n",
        "df_clean = df_computers.copy()\n",
        "print(\"Starting cleaning process...\")\n",
        "print(f\"Initial shape: {df_clean.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Clean Multilabel Fields\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean multilabel fields\n",
        "print(\"=\" * 80)\n",
        "print(\"7.1 Cleaning Multilabel Fields\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Preserve some columns if needed (like 'Ofertas' which might be informative)\n",
        "preserve_cols = ['Ofertas']  # Add any columns you want to preserve\n",
        "\n",
        "for col in multilabel_cols:\n",
        "    if col not in preserve_cols:\n",
        "        before_sample = df_clean[col].dropna().iloc[0] if len(df_clean[col].dropna()) > 0 else None\n",
        "        df_clean[col] = clean_multilabel_series(df_clean[col])\n",
        "        after_sample = df_clean[col].dropna().iloc[0] if len(df_clean[col].dropna()) > 0 else None\n",
        "        if before_sample != after_sample and before_sample is not None:\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(f\"  Before: '{before_sample}'\")\n",
        "            print(f\"  After:  '{after_sample}'\")\n",
        "\n",
        "print(f\"\\nCleaned {len(multilabel_cols)} multilabel columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Clean Storage Fields\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean storage fields\n",
        "print(\"=\" * 80)\n",
        "print(\"7.2 Cleaning Storage Fields\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "config = get_default_config()\n",
        "df_clean = clean_storage_fields(\n",
        "    df_clean,\n",
        "    ssd_col=config['storage']['ssd_col'],\n",
        "    hdd_col=config['storage']['hdd_col'],\n",
        "    storage_type_col=config['storage']['storage_type_col']\n",
        ")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\nStorage cleaning examples:\")\n",
        "if 'storage_total_gb' in df_clean.columns:\n",
        "    sample = df_clean[['Disco duro_Capacidad de memoria SSD', 'ssd_gb', 'storage_total_gb']].dropna().head(5)\n",
        "    print(sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Clean Screen Fields\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean screen fields\n",
        "print(\"=\" * 80)\n",
        "print(\"7.3 Cleaning Screen Fields\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_clean = clean_screen_fields(\n",
        "    df_clean,\n",
        "    size_col=config['screen']['size_col'],\n",
        "    resolution_col=config['screen']['resolution_col']\n",
        ")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\nScreen cleaning examples:\")\n",
        "if 'screen_size_inches' in df_clean.columns:\n",
        "    sample = df_clean[[config['screen']['size_col'], 'screen_size_inches']].dropna().head(5)\n",
        "    print(sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4 Clean CPU/GPU Fields\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean CPU/GPU fields\n",
        "print(\"=\" * 80)\n",
        "print(\"7.4 Cleaning CPU/GPU Fields\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_clean = create_cleaned_cpu_gpu_columns(\n",
        "    df_clean,\n",
        "    cpu_col=config['cpu_gpu']['cpu_col'],\n",
        "    gpu_col=config['cpu_gpu']['gpu_col']\n",
        ")\n",
        "\n",
        "# Show examples\n",
        "if 'cpu_clean' in df_clean.columns:\n",
        "    print(\"\\nCPU cleaning examples:\")\n",
        "    sample = df_clean[[config['cpu_gpu']['cpu_col'], 'cpu_clean']].dropna().head(5)\n",
        "    print(sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.5 Extract Numeric Values from Other Fields\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract numeric from other fields\n",
        "print(\"=\" * 80)\n",
        "print(\"7.5 Extracting Numeric Values\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "numeric_fields = [\n",
        "    ('RAM_Memoria RAM', 'ram_gb'),\n",
        "    ('Procesador_Frecuencia de reloj', 'cpu_freq_ghz'),\n",
        "    ('Medidas y peso_Peso', 'weight_kg'),\n",
        "    ('Alimentación_Vatios-hora', 'battery_wh'),\n",
        "]\n",
        "\n",
        "for col, new_col in numeric_fields:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[new_col] = extract_numeric_series(df_clean[col])\n",
        "        print(f\"Created {new_col} from {col}\")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\nNumeric extraction examples:\")\n",
        "if 'ram_gb' in df_clean.columns:\n",
        "    sample = df_clean[['RAM_Memoria RAM', 'ram_gb']].dropna().head(5)\n",
        "    print(sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.6 Missing Value Imputation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing value imputation\n",
        "print(\"=\" * 80)\n",
        "print(\"7.6 Missing Value Imputation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Numeric columns: median imputation\n",
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "imputed_numeric = 0\n",
        "for col in numeric_cols:\n",
        "    if df_clean[col].isna().any():\n",
        "        median_val = df_clean[col].median()\n",
        "        if pd.notna(median_val):\n",
        "            missing_count = df_clean[col].isna().sum()\n",
        "            df_clean[col].fillna(median_val, inplace=True)\n",
        "            imputed_numeric += missing_count\n",
        "\n",
        "print(f\"Imputed {imputed_numeric} missing values in numeric columns using median\")\n",
        "\n",
        "# Categorical columns: mode imputation or \"Unknown\"\n",
        "categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
        "imputed_categorical = 0\n",
        "for col in categorical_cols:\n",
        "    if df_clean[col].isna().any():\n",
        "        mode_val = df_clean[col].mode()\n",
        "        if len(mode_val) > 0:\n",
        "            missing_count = df_clean[col].isna().sum()\n",
        "            df_clean[col].fillna(mode_val[0], inplace=True)\n",
        "            imputed_categorical += missing_count\n",
        "        else:\n",
        "            missing_count = df_clean[col].isna().sum()\n",
        "            df_clean[col].fillna('Unknown', inplace=True)\n",
        "            imputed_categorical += missing_count\n",
        "\n",
        "print(f\"Imputed {imputed_categorical} missing values in categorical columns using mode/Unknown\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "remaining_missing = df_clean.isnull().sum().sum()\n",
        "print(f\"\\nRemaining missing values: {remaining_missing}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plots for key numeric fields\n",
        "if 'storage_total_gb' in df_clean.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    df_clean['storage_total_gb'].hist(bins=50, edgecolor='black')\n",
        "    plt.title('Distribution of Total Storage (GB)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Storage (GB)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../docs/storage_distribution.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'screen_size_inches' in df_clean.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    df_clean['screen_size_inches'].hist(bins=30, edgecolor='black')\n",
        "    plt.title('Distribution of Screen Size (inches)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Screen Size (inches)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../docs/screen_size_distribution.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'ram_gb' in df_clean.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    df_clean['ram_gb'].hist(bins=30, edgecolor='black')\n",
        "    plt.title('Distribution of RAM (GB)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('RAM (GB)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../docs/ram_distribution.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "os.makedirs('../data/clean', exist_ok=True)\n",
        "\n",
        "# Export cleaned dataset\n",
        "output_path = '../data/clean/db_computers_cleaned.csv'\n",
        "df_clean.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CLEANED DATASET EXPORTED\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nOutput path: {output_path}\")\n",
        "print(f\"Final shape: {df_clean.shape}\")\n",
        "print(f\"Original shape: {df_computers.shape}\")\n",
        "print(f\"\\nNew columns created:\")\n",
        "new_cols = [col for col in df_clean.columns if col not in df_computers.columns]\n",
        "for col in new_cols:\n",
        "    print(f\"  - {col}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary Statistics\n",
        "\n",
        "Final summary of the cleaned dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for key numeric fields\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY STATISTICS - CLEANED DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summary_cols = ['storage_total_gb', 'ssd_gb', 'hdd_gb', 'screen_size_inches', \n",
        "                'ram_gb', 'cpu_freq_ghz', 'weight_kg', 'battery_wh']\n",
        "available_cols = [col for col in summary_cols if col in df_clean.columns]\n",
        "\n",
        "if available_cols:\n",
        "    print(\"\\nNumeric Summary:\")\n",
        "    print(df_clean[available_cols].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA QUALITY SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nOriginal dataset:\")\n",
        "print(f\"  Rows: {df_computers.shape[0]}\")\n",
        "print(f\"  Columns: {df_computers.shape[1]}\")\n",
        "print(f\"  Missing values: {df_computers.isnull().sum().sum()}\")\n",
        "\n",
        "print(f\"\\nCleaned dataset:\")\n",
        "print(f\"  Rows: {df_clean.shape[0]}\")\n",
        "print(f\"  Columns: {df_clean.shape[1]}\")\n",
        "print(f\"  Missing values: {df_clean.isnull().sum().sum()}\")\n",
        "\n",
        "print(f\"\\nCleaning operations completed:\")\n",
        "print(f\"  ✓ Multilabel fields cleaned: {len(multilabel_cols)}\")\n",
        "print(f\"  ✓ Storage fields standardized\")\n",
        "print(f\"  ✓ Screen fields normalized\")\n",
        "print(f\"  ✓ CPU/GPU fields cleaned for matching\")\n",
        "print(f\"  ✓ Numeric values extracted\")\n",
        "print(f\"  ✓ Missing values imputed\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
